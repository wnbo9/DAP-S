%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%
\newtheorem{proposition}[theorem]{Proposition}%
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%
\raggedbottom



\begin{document}

\title[Article Title]{DAP-PIR: Efficient Genetic Association Analysis through Pseudo Importance Resampling}

\author[1]{\fnm{Bo} \sur{Wang}}\email{wnbo@umich.edu}

\author*[1]{\fnm{Xiaoquan} \sur{Wen}}\email{xwen@umich.edu}

\affil[1]{\orgdiv{Department of Biostatistics}, \orgname{University of Michigan}, \orgaddress{\city{Ann Arbor}, \state{MI} \postcode{48109}, \country{USA}}}

\abstract{In genome-wide association studies (GWAS), identifying causal variants from a vast number of genetic variants is challenging. This paper presents DAP-PIR, a method combining Deterministic Approximation of Posteriors (DAP) with Pseudo-Importance Resampling (PIR), to enhance computational efficiency and improve the accuracy of fine-mapping in genetic studies. By selecting a limited number of high-probability models, DAP-PIR effectively narrows down the candidate models, maintaining accuracy in posterior estimation while reducing computational costs. Simulations and real data analyses show the superiority of DAP-PIR over existing methods, including SuSiE, particularly in managing high-dimensional genetic data.}


\keywords{Genetic fine mapping, variable selection, importance sampling, variational inference}

\maketitle



\section{Introduction}\label{sec1}
Genome-wide association studies (GWAS) have successfully uncovered thousands of genetic variants associated with complex traits and diseases \cite{visscher201710}. However, due to the intricate linkage disequilibrium (LD) structure in the human genome, many variants identified in GWAS are merely correlated with true causal variants rather than causative themselves \cite{ardlie2002patterns}. Fine mapping analysis is therefore essential to refine these associations and identify specific genetic variants likely to be causal \cite{spain2015strategies, schaid2018genome}.

To address this challenge, Bayesian variable selection regression (BVSR) methods are widely employed to account for the uncertainty in identifying candidate causal variants. Traditional methods like CAVIAR rely on exhaustive searches \cite{hormozdiari2014identifying}, while DAP-G employs a boosting approach to prioritize variants, which, although effective, can sometimes be slow in practice \cite{wen2016efficient, lee2018bayesian}. Alternatively, the SuSiE method offers an efficient, variational approach to prioritize SNPs based on posterior inclusion probabilities (PIP) \cite{wang2020simple}. However, SuSiE can occasionally experience accuracy issues with PIP, particularly in complex LD structures.

Here, we propose DAP-PIR, which combines the strengths of both DAP and SuSiE to achieve efficient and accurate deterministic approximation of posteriors. By leveraging SuSiE's variational approximation results, DAP-PIR efficiently prioritizes models for deterministic posterior estimation. This approach ensures computational efficiency while providing a robust solution to fine mapping challenges in GWAS. Through both simulations and real data applications, we demonstrate DAP-PIRâ€™s advantages over existing methods in terms of both speed and accuracy, making it an effective tool for fine mapping causal variants in GWAS.



\section{Results}\label{sec2}
\subsection{Overview of the DAP-PIR algorithm}\label{subsec1}

\subsection{Comparison of fine mapping methods}\label{subsec3}
We conduct several simulation studies to show the accuracy and efficiency of the DAP-PIR algorithm. Comparing the (1) power; (2) coverage; (3) PIP calibration; (4) computational efficiency among fine mapping methods.

\subsection{Real data results}\label{subsec4}
We analyze some real data using various fine mapping methods.

\section{Discussion}\label{sec3}



\section{Methods}\label{sec4}
\subsection{DAP-PIR}\label{subsec1}
\subsubsection{Bayesian variable selection model}\label{subsubsec1}
We consider the context of association analysis for a single quantitative trait. Let $\mathbf{y}=\left(y_1,y_2,\cdots,y_n\right)^T$ denote an $n$-dimensional vector representing the trait of interest, $\mathbf{G} = \left(\mathbf{g}_1,\mathbf{g}_2,\cdots,\mathbf{g}_p\right)$ represent an $n \times p$ genotype matrix, where each column $\mathbf{g}_j$ is an $n$-dimensional vector of genotypes for the $j$-th variant. We assume that $\mathbf{y}$ and the columns of $\mathbf{G}$ are centered to have a mean of zero to avoid the need for an intercept term. The model is then defined as:
\begin{equation}
    \mathbf{y} = \sum_{j=1}^{p} \beta_j \mathbf{g}_j  + \mathbf{e}, \; \mathbf{e} \sim N_n\left(0, \tau^{-1}I_n\right),
\end{equation}
where $\beta_j$ denotes the genetic effect of the $j$-th variant, and $\tau$ represents the precision. The goal of GWAS fine mapping is to prioritize causal variants rather than estimate their effect sizes, therefore we introduce an inclusion indicator $\gamma_j = I(\beta_j \neq 0)$ for each variant, where $\gamma_j=1$ indicates that the $j$-th variant is causal. The primary aim is infer the posterior distribution of the causal status vector: $\boldsymbol{\gamma} = \left(\gamma_1,\gamma_2,\cdots,\gamma_p\right)^T$. While evaluating each possible model configuration $\boldsymbol{\gamma}$ is straightforward as it only requires integrating out $\beta_j$'s and $\tau$, summing over all $2^p$ model configurations becomes computationally intractable as $p$ increases. This makes traditional Markov Chain Monte Carlo (MCMC) methods inefficient for fine mapping tasks involving even a moderate number of variants. Therefore, we utilize the efficient pseudo importance resampling method to explore the model space and approximate the posterior distribution of $\boldsymbol{\gamma}$.



\subsubsection{Pseudo importance resampling}\label{subsubsec3}
For a discrete random variable $\mathbf{X}$ with target distribution $p\left(\mathbf{x}\right)$, the objective is to approxiamte the expectation $\mathbb{E}_{p}[f\left( \mathbf{X} \right)]$ for a function $f\left(\mathbf{x}\right) = \mathbf{x}$. Due to the high-dimensional sample space, exact computation is impractical. We employ Importance Sampling to estimate this expectation by sampling from a proposal distribution $q\left(\mathbf{x}\right)$ and assigning importance weights $w\left( \mathbf{x}^{i} \right) = p\left(\mathbf{x}^{i}\right)/q\left(\mathbf{x}^{i}\right)$ for samples $\mathbf{x}^i \sim q\left(\mathbf{x}\right)$. The IS estimate is:
\begin{equation}
    \tilde{\boldsymbol{\mu}}_{\operatorname{IS}} =  \frac{\sum_{i=1}^{n}w\left( \mathbf{x}^{i} \right) f\left( \mathbf{x}^{i} \right)}{\sum_{i=1}^n w\left( \mathbf{x}^{i} \right)}.
\end{equation}
To obtain a reliable estimate, $q\left(\mathbf{x}\right)$ should closely approximate $p\left(\mathbf{x}\right)$, particularly in regions where $f\left(\mathbf{x}\right)p\left(\mathbf{x}\right)$ has significant values.

We use Variational Approximation (VA) to construct an effective proposal distribution by minimizing the Kullback-Leibler (KL) divergence between $p\left(\mathbf{x}\right)$ to $q\left(\mathbf{x}\right)$. VA provides a tractable approxiamtion, balancing efficiency and accuracy. The Variational Importance Sampling (VIS) method integrates VA with IS, leveraging VA's computational efficiency while reducing bias through IS.

\begin{algorithm}
    \caption{Variational Importance Sampling (VIS)}
    \label{alg:variational_is}
    \begin{enumerate}
        \item Target distribution: derive the analytical expression of \(p'(\mathbf{x})\) up to a normalizing constant.
        \item Proposal distribution: obtain the variational approximation \( q(\mathbf{x}) \) to \( p(\mathbf{x}) \) within the family $\mathcal{Q}$.
        \item Sampling: for \( i \in \{1, \dots, n\} \),
        \begin{enumerate}
            \item draw \( \mathbf{x}^{i} \) from the proposal distribution \( q(\mathbf{x}) \).
            \item calculate the function value $f\left(\mathbf{x}^{i}\right)$.
            \item calculate the importance weight \( w\left(\mathbf{x}^{i}\right) = p'\left(\mathbf{x}^{i}\right) / q\left(\mathbf{x}^{i}\right) \).
        \end{enumerate}
        \item Estimation: estimating $\mathbb{E}_{p}[f\left( \mathbf{X} \right)]$ by \( \tilde{\boldsymbol{\mu}}_{\operatorname{IS}}=\frac{\sum_{i=1}^{n}w\left( \mathbf{x}^{i} \right) f\left( \mathbf{x}^{i} \right)}{\sum_{i=1}^n w\left( \mathbf{x}^{i} \right)}\).
    \end{enumerate}
\end{algorithm}



Sampling Importance Resampling (SIR) selects IS-generated samples proportional to their importance weights to approximate $p\left(\mathbf{x}\right)$ \cite{rubin1987calculation, db1988using}. Given $n$ samples $\left\{ \left(\mathbf{x}^{i}, w\left(\mathbf{x}^{i}\right)\right): i=1,\cdots,n\right\}$, SIR resamples $m$ values from them according to their importance weights:
\begin{equation}
    \operatorname{Pr}\left[\mathbf{x}^{*} = \mathbf{x}^{k}\right] = \frac{w\left(\mathbf{x}^{k}\right)}{\sum_{i=1}^{n}w\left(\mathbf{x}^{i}\right)}.
\end{equation}
The resampled set $\left\{\mathbf{x}^{*j}: j=1,\cdots,m\right\}$ has equal weights and represents the target distribution $p$. The SIR algorithm is outlines in Algorithm \ref{alg:variational_sir}. The rationale behind SIR is based on the fact that as $n/m$ tends to infinity, the $m$ samples are drawn with probabilities given by:
$$p^*\left(\mathbf{x}\right) \propto q\left(\mathbf{x}\right)w\left(\mathbf{x}\right) \propto q\left(\mathbf{x}\right)\frac{p\left(\mathbf{x}\right)}{q\left(\mathbf{x}\right)} = p\left(\mathbf{x}\right),$$
which shows that the SIR algorithm generates independent and identically distributed (i.i.d) samples from the target distribution $p\left(\mathbf{x}\right)$, as desired. However, drawing such a large number of samples can be computationally expensive. To balance computational efficiency and performance, Rubin suggested using $n/m=20$ as a practical ratio for resampling, which provides adequate performance without requiring excessive duplicates in the resampling \cite{rubin1987calculation}.

\begin{algorithm}
\caption{Sampling Importance Resampling (SIR)}
\label{alg:variational_sir}
\begin{enumerate}
    \item Target distribution: the distribution $p\left(\mathbf{x}\right)$ with the expression of \(p'\left(\mathbf{x}\right)\) up to a normalizing constant.
    \item Proposal distribution: the variational approximation \( q\left(\mathbf{x}\right) \) to \( p\left(\mathbf{x}\right) \) within the family $\mathcal{Q}$.
    \item Sampling: for \( i \in \{1, \dots, n\} \),
    \begin{enumerate}
        \item draw \( \mathbf{x}^{i} \) from the proposal distribution \( q\left(\mathbf{x}\right) \).
        \item calculate the importance weight \( w\left(\mathbf{x}^{i}\right) = p'\left(\mathbf{x}^{i}\right) / q\left(\mathbf{x}^{i}\right) \).
    \end{enumerate}
    \item Resampling: for \( j \in \{1, \dots, m\} \),
    \begin{enumerate}
        \item resample \( \mathbf{x}^{*j} \) from the samples $\left\{\mathbf{x}^{i}: i=1,\cdots,n\right\}$ with probabilities proportional to the importance weights.
        \item include \( \mathbf{x}^{*j} \) in the resampled set.
    \end{enumerate}
    \item Result: the resampled set $\left\{\mathbf{x}^{*j}: j=1,\cdots,m\right\}$ which is approximately distributed according to the target distribution $p\left(\mathbf{x}\right)$.
\end{enumerate}
\end{algorithm}

To enhance efficiency, we introduce Pseudo Importance Resampling (PIR), which eliminates explicit resampling by shrinking the proposal distribution. PIR focuses on regions where $p\left(\mathbf{x}\right)$ has meaningful contributions, reducing computational cost in high-dimensional settings.

In practice, if a value of $\mathbf{x}$ exhibits a near-zero probability density under the target distribution $p\left(\mathbf{x}\right)$, its contribution to the final result becomes negligible. In such cases, the proposal distribution can be effectively reduced to zero for that region, excluding it from further consideration. This reduction in the number of regions to explore allows us to focus computational resources on areas where $p\left(\mathbf{x}\right)$ significantly impacts the result. Prioritizing these high-priority regions improves efficiency in high-dimensional settings, making the process more computational feasible. The KL divergence penalizes cases where $p$ is large while $q\left(\mathbf{x}\right)$ is small, implying that when $q\left(\mathbf{x}\right)$ is small, it is highly probable that $p\left(\mathbf{x}\right)$ is also small. As a result, we can safely reduce the sample space by identifying and excluding values of $\mathbf{x}$ that are negligible under the proposal distribution. Even in cases where $p\left(\mathbf{x}\right)$ is small but $q\left(\mathbf{x}\right)$ is large (and thus not excluded), the contribution to the overall result remains minimal due to the small value of $p\left(\mathbf{x}\right)$.

For a specific point $\mathbf{x}^{(k)}$ in the target distribution, the proposal distribution shrinkage is defined as:
$$q'\left(\mathbf{x}^{(k)}\right) = \begin{cases} 
    0, & \text{if } q\left(\mathbf{x}^{(k)}\right)<\epsilon, \\
    q\left(\mathbf{x}^{(k)}\right), & \text{if } q\left(\mathbf{x}^{(k)}\right)\geq \epsilon, \end{cases}
$$
where $\epsilon$ is a threshold value. The normalized proposal distribution is given by:
$$q^*\left(\mathbf{x}^{(k)}\right) = \frac{q'\left(\mathbf{x}^{(k)}\right)}{\sum_{i=1}^{N} q'\left(\mathbf{x}^{(i)}\right)}=\frac{q'\left(\mathbf{x}^{(k)}\right)}{\sum_{j=1}^{M} q\left(\mathbf{x}^{(j)}\right)},$$
where $\mathbf{x}^{(j)}$ corresponds to points with non-zero proposal density, $N$ is the total number of possible values, and $M$ is the number of points with non-zero proposal density. During the SIR process, the probability of selecting a value $\mathbf{x}^{(k)}$ in the reduced sample space is calculated as:
$$\begin{aligned}
    \operatorname{Pr}\left(\mathbf{x}^* = \mathbf{x}^{(k)}\right) & = \frac{w\left(\mathbf{x}^{(k)}\right)\#\left(\mathbf{x}^{(k)}\right)}{\sum_{i=1}^{N}w\left(\mathbf{x}^{(i)}\right)\#\left(\mathbf{x}^{(i)}\right)} \\
    &= \frac{w\left(\mathbf{x}^{(k)}\right)\#\left(\mathbf{x}^{(k)}\right)}{\sum_{j=1}^{M}w\left(\mathbf{x}^{(j)}\right)\#\left(\mathbf{x}^{(j)}\right)}\\
    &\xrightarrow{n \to \infty} \frac{\frac{p'\left(\mathbf{x}^{(k)}\right)}{q^*\left(\mathbf{x}^{(k)}\right)}q^*\left(\mathbf{x}^{(k)}\right)\times n}{\sum_{j=1}^{M}\frac{p'\left(\mathbf{x}^{(j)}\right)}{q^*\left(\mathbf{x}^{(j)}\right)}q^*\left(\mathbf{x}^{(j)}\right)\times n} \\
    &= \frac{p'\left(\mathbf{x}^{(k)}\right)}{\sum_{j=1}^{M}p'\left(\mathbf{x}^{(j)}\right)},
\end{aligned}$$
where $\#\left(\mathbf{x}^{(k)}\right)$ is the number of samples with value equal to $\mathbf{x}^{(k)}$. Thus, instead of drawing an large number of $n$ samples and resampling an adequate number of $m$ from them, we can directly approximate the probability of each value in the target distribution by focusing only on the $M$ points with non-negligible probability. This process, based on SIR but avoiding the separate steps of sampling and resampling, is referred to as Pseudo Importance Resampling (PIR) outlined in Algorithm \ref{alg:variational_pir}. PIR effectively reduces the sample space by shrinking the proposal distribution and concentrating on the regions that matter most, providing a good approximation to the target distribution.

\begin{algorithm}
\caption{Pseudo Importance Resampling (PIR)}
\label{alg:variational_pir}
\begin{enumerate}
    \item Target distribution: the distribution $p\left(\mathbf{x}\right)$ with the expression of \(p'\left(\mathbf{x}\right)\) up to a normalizing constant.
    \item Proposal distribution: the variational approximation \( q\left(\mathbf{x}\right) \) to \( p\left(\mathbf{x}\right) \) within the family $\mathcal{Q}$.
    \item Proposal distribution shrinkage: shrink $q\left(\mathbf{x}\right)$ to obtain $q^*\left(\mathbf{x}\right)$, reducing the sample space to $M$ non-negligible partitions.
    \item Pseudo resampling: for \( j \in \{1, \dots, M\} \),
    \begin{enumerate}
        \item calculate the probability score \( p'\left(\mathbf{x}^{(j)}\right) \).
        \item calculate the approximation \( p'\left(\mathbf{x}^{(j)}\right)/ \sum_{j=1}^{M}p'\left(\mathbf{x}^{(j)}\right) \).
    \end{enumerate}
\end{enumerate}
\end{algorithm}

\subsubsection{Deterministic approximation of poseteriors through PIR}\label{subsubsec4}
The posterior probability of one model configuration $\boldsymbol{\gamma}^{(i)}$ can be derived using Bayes' theorem:
$$p\left(\boldsymbol{\gamma} = \boldsymbol{\gamma}^{(i)} \mid \mathbf{G},\mathbf{y}\right)     =\frac{\pi\left(\boldsymbol{\gamma}^{(i)}\right)\operatorname{BF}\left(\boldsymbol{\gamma}^{(i)}\right)}{\sum_{\boldsymbol{\gamma}'\in \Gamma}\pi \left(\boldsymbol{\gamma}'\right)\operatorname{BF}\left(\boldsymbol{\gamma}'\right)},$$
where $\Gamma$ denotes all $2^p$ possible model configurations, $\pi\left(\boldsymbol{\gamma}^{(i)}\right)$ is the prior probability of model $\boldsymbol{\gamma}^{(i)}$, and $\operatorname{BF}\left(\boldsymbol{\gamma}^{(i)}\right)$ is the Bayes factor, which measures the marginal likelihood of $\boldsymbol{\gamma}$ evaluated at $\boldsymbol{\gamma}^{(i)}$. The posterior inclusion probability (PIP) for each variant is then calculated by marginalizing the posterior probabilities across all model configurations:
$$\operatorname{PIP}_j:=\operatorname{Pr}\left(\gamma_j=1|\mathbf{G},\mathbf{y}\right)=\sum_{\boldsymbol{\gamma}':\gamma_j=1} p(\boldsymbol{\gamma}=\boldsymbol{\gamma}'|\mathbf{G},\mathbf{y}).$$


Under the $D_2$ prior, the prior distribution for $\tau$ is given by:
$$\tau \sim \Gamma\left(\kappa/2, \lambda/2\right),$$
where the limiting form of the prior is obtained as $\kappa,\lambda \rightarrow 0$. And we assign a spike-and-slab prior for the effect size $\beta_j$:
$$\beta_j \mid \gamma_j \sim (1-\gamma_j)\delta_0 + \gamma_j N\left(0, \phi^2/\tau\right),$$
where $\delta_0$ refers to Dirac's delta function, and $\phi^2$ is the prior variance of the effect size of the causal variant. In the computation of the Bayes factor, we use $\phi^2=0.6^2$. For a specific value of $\phi^2$ and a model configuration $\boldsymbol{\gamma}=\left(\gamma_1,\cdots,\gamma_{p}\right)$, the limiting of the Bayes factor is calculated by:
\begin{equation}
    \lim _{\substack{\kappa \rightarrow 0 \\ \lambda \rightarrow 0}} \operatorname{BF}(\boldsymbol{\gamma})= \frac{1}{|\phi^{-2}I + \mathbf{X}^T\mathbf{X}|^{1 / 2}}  \frac{1}{\phi} \left[\frac{\mathbf{y}^T\left(I-\mathbf{X}\left(\phi^{-2}I+\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T\right) \mathbf{y}}{\mathbf{y}^T \mathbf{y}}\right]^{-n / 2},
\end{equation}
where $\mathbf{X}$ represents genotypes indicated by non-zero entry in $\boldsymbol{\gamma}$.




\subsection{Simulations}\label{subsec2}
\subsubsection{Simulation with normal distribution}
To evaluate the performance of DAP-PIR, we conduct a simulation study with a normal distribution for the genotype data. Specifically, we consider a scenario with $n=500$ individuals and $p=10$ SNPs to relive the computational burden of exact computation of the posterior. Each SNP $g_{ij}$ is generated independently from a standard normal distribution. We randomly select one to five variants and specify them as causal variants with the setting:
\begin{align}
    g_{ij} &\sim \operatorname{N}\left(0,1\right),\\
    y_i &= \sum_{j=1}^{10} \beta_j g_{ij}  + \epsilon_i,\\
    \epsilon_i &\sim \operatorname{N}(0, \tau^{-1}),\\
    \beta_j  &\sim (1-\gamma_j)\delta_0 + \gamma_j\operatorname{N}\left(0, \sigma^2_{0}/\tau\right),
\end{align}
where $\beta_j$ represents the effect size of the $j$-th SNP, $\epsilon_i$ is the residual term with varaince $\tau^{-1}$, and $\sigma_0^2/\tau$ is the prior effect size variance. The effect sizes $\beta_j$ are determined by a binary indicator $\gamma_j$: if $\gamma_j=0$, $\beta_j$ is set to zero and indicates a non-causal SNP; if $\gamma_j=1$, $\beta_j$ is drawn from a normal distribution with mean zero and variance $\sigma^2_0/\tau$, representing a causal SNP. We randomly assign $S$ causal variants, and the indicator vector $\boldsymbol{\gamma} = \left(\gamma_1,\cdots,\gamma_{10}\right)$ is a 10-vector of binary variables such that only $S$ entries are 1 and the other entries are 0. In the simulation, we set $\tau=1$, $\sigma^2_0 = 0.6^2$, and choose $S \in \left\{1,2,3,4,5\right\}$. For each setting, we simulate 1000 times. Then we have simulated $1,000\times 5 = 5,000$ datasets for further investigation.


\subsubsection{Simulation on GTEx V8 data}
Using the GTEx V8 data \cite{gtex2015genotype} with $n=670$ individual, we select 1,000 genes and $p=1,000$ SNPs near the region of each selected gene. We simulate the phenotypes with various combinations of the number of effects, $S$, and proportion of variance explained (PVE) by genotypes, $\phi$. The simulation settings are as follows:
\begin{align}
    y_i &= \sum_{j=1}^{1000} \beta_j g_{ij}  + \epsilon_i,\\
    \epsilon_i &\sim \operatorname{N}(0, \sigma^2),\\
    \sigma^2 &= \frac{1-\phi}{\phi}\text{Var}(\mathbf{G}\boldsymbol{\beta}),\\
    \beta_j  &\sim (1-\gamma_j)\delta_0 + \gamma_j\operatorname{N}\left(0, 0.6^2\right).
\end{align}
Specifically, for each gene, we randomly assign $S$ variants to be causal, while their effect sizes are independently drawn from $\operatorname{N}(0, 0.6^2)$ and set the other effect sizes to zero. The variance of the error term $\sigma^2$ is given by $\frac{1-\phi}{\phi}\text{Var}(\mathbf{G}\boldsymbol{\beta})$ and the simulated phenotypes follow $\operatorname{N}(\mathbf{G}\boldsymbol{\beta}, \sigma^2)$. We follow the SuSiE setting and generate data with pairwise combinations of $S \in \{1,2,3,4,5\}$ and $\phi \in \{0.05,0.1,0.2,0.4\}$. Then we have simulated $1,000\times 5 \times 4 = 20,000$ datasets for further investigation.



\subsection{Real data application}\label{subsec3}





\newpage
\section{Figures and tables}\label{sec6}



\begin{algorithm}
\caption{Calculate $y = x^n$}\label{algo1}
\begin{algorithmic}[1]
\Require $n \geq 0 \vee x \neq 0$
\Ensure $y = x^n$ 
\State $y \Leftarrow 1$
\If{$n < 0$}\label{algln2}
        \State $X \Leftarrow 1 / x$
        \State $N \Leftarrow -n$
\Else
        \State $X \Leftarrow x$
        \State $N \Leftarrow n$
\EndIf
\While{$N \neq 0$}
        \If{$N$ is even}
            \State $X \Leftarrow X \times X$
            \State $N \Leftarrow N / 2$
        \Else[$N$ is odd]
            \State $y \Leftarrow y \times X$
            \State $N \Leftarrow N - 1$
        \EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}





\newpage
\bibliography{references}

\end{document}
